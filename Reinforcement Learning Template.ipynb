{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25722b33-90b5-46cb-86ac-39fef537f8d0",
   "metadata": {},
   "source": [
    "## Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e3ec1f0-a3db-4027-b19b-001a061503e1",
   "metadata": {},
   "source": [
    "This notebook aims to be a minimalsit template for building reinfocement learning environments. Code which is very re-usable is provided, but gaps are left in most places where you need to write code which is specific to your problem. \n",
    "\n",
    "The main sections are:-\n",
    "- system parameters: this is where any constants or simple functions required for simulation are coded.\n",
    "- notebook parameters: general control of this notebook, whether to save results, display outputss etc.\n",
    "- system constructor: the main body of code defining the environment and handlers for updating it and similar.\n",
    "- agent handler: a constructor used for managing the agents in the simulated environment, data mining etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5cb084c-8825-402e-9fcf-980422899658",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848c8a05-83d3-49eb-8ab9-1f6b9dc1f8fa",
   "metadata": {},
   "source": [
    "I will create these:-\n",
    "\n",
    "A general system object capable of representing arbitrary reinforcement learning environments, with the following attributes:-\n",
    "- constructor\n",
    "- copySystem\n",
    "- readInputs\n",
    "- generateRandomAction\n",
    "- generateRandomState\n",
    "- interpretAction\n",
    "- setAction\n",
    "- updateSystem\n",
    "\n",
    "I will also create a generalized agent which works as follows:-\n",
    "- takes the system in a particular state\n",
    "- uses readInputs to identify the current state\n",
    "- passes readInputs to some decision algorithm\n",
    "- outputs the decision in the format required by interpetAction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd23d62f-93a7-4923-af60-0b941e9e2f14",
   "metadata": {},
   "source": [
    "## To-Do"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2cce03-ad6d-407a-82d1-7f5dd4f6973d",
   "metadata": {},
   "source": [
    "none"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd1779a-fa56-4af8-bffd-5d9cb9fc324a",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f107a743-a84c-4713-8efc-dd9d86bdd418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random as r\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a35419a6-880f-4a15-9aff-0f442a92f7ec",
   "metadata": {},
   "source": [
    "## System Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b116ee-2cef-46f8-b4c5-9abc93fcc269",
   "metadata": {},
   "source": [
    "These are problem-specific parameters and functions. Anything which is particular to this RL problem but not part of the environment definition goes here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa98b3e1-5dbb-46e1-ab63-d9f1fbf6a923",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d114ad86-ea9d-41c2-abb5-37c0bbbaf30f",
   "metadata": {},
   "source": [
    "## Notebook parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5106158e-e3e1-45ed-a1f7-fdde369c022b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Display\n",
    "verbosity = False #Whether neural networks should display their predictions and training\n",
    "\n",
    "#Training parameters\n",
    "\n",
    "#Testing parameters\n",
    "\n",
    "#Whether to seed the notebook's randomness\n",
    "seed = True\n",
    "if seed:\n",
    "    r.seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec8eeec-3219-465a-8b4e-5e9aa9692b63",
   "metadata": {},
   "source": [
    "## System Constructor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ed3568-9588-4832-815c-b847a525d9a8",
   "metadata": {},
   "source": [
    "All RL problems will need some version of these functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd3d819d-cf92-4ff7-86a2-c714c70eb55c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 2 (4505655.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[4], line 7\u001b[0;36m\u001b[0m\n\u001b[0;31m    def readInputs(self):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block after function definition on line 2\n"
     ]
    }
   ],
   "source": [
    "class System:\n",
    "    def __init__(\n",
    "        self, \n",
    "    ):\n",
    "\n",
    "    #Given the state of the environmnent, what do the agents actually see? \n",
    "    def readInputs(self):\n",
    "        output = [\n",
    "        ]\n",
    "        return(output)\n",
    "\n",
    "    #Creates a dummy copy of the system - useful for constructing other functions\n",
    "    def copySystem(self):\n",
    "        dummySystem = System(\n",
    "        )\n",
    "        return(dummySystem)\n",
    "\n",
    "    #Generates any random action, ignoring validity constraints\n",
    "    def generateRandomAction(self):\n",
    "        output = [\n",
    "        ]\n",
    "        return(output)\n",
    "\n",
    "    #Creates a randomly chosen state with uniform distribution\n",
    "    def generateRandomState(self):\n",
    "        dummySystem = System(\n",
    "        )\n",
    "        return(dummySystem)\n",
    "\n",
    "    #Creates a randomly chosen state, excluding extremes. Not needed for all problems, but can be useful for managing edge cases\n",
    "    def generateRandomMiddleState(self):\n",
    "        dummySystem = System(\n",
    "        )\n",
    "        return(dummySystem)\n",
    "\n",
    "    #Creates a default state: useful if there's a particularly common state such as initial configurations\n",
    "    def generateDefaultState(self):\n",
    "        dummySystem = System()\n",
    "        return(dummySystem)\n",
    "\n",
    "    #Checks agent decisions for validity and interprets invalid actions\n",
    "    def interpretAction(\n",
    "        self,\n",
    "    ):\n",
    "        return(output)\n",
    "\n",
    "    #Some problems have actions as parts of the environment (e.g. opening or closing a valve). If so, setAction handles this.\n",
    "    def setAction(\n",
    "        self    \n",
    "    ):\n",
    "        return(dummySystem)\n",
    "\n",
    "    #One \"turn\" might be literally the agent's turn in a discrete time game, or some small unit of time (e.g. 1 second) in continuous time\n",
    "    def updateSystemOneTurn(self):\n",
    "\n",
    "        outputSystem = System(\n",
    "        )\n",
    "        return(outputSystem)\n",
    "\n",
    "    #For updating the system for multiple turns/timesteps, or if a different agent gets a turn after our agent (e.g. chess, go)\n",
    "    def updateSystem(self):\n",
    "        outputSystem = System(\n",
    "        )\n",
    "        return(outputSystem)\n",
    "\n",
    "    '''\n",
    "    The next functions are useful for interpreting data but not strictly needed to run the agents\n",
    "    '''\n",
    "    def readData(self):\n",
    "        output = [\n",
    "        ]\n",
    "        return(output)\n",
    "        \n",
    "    #How good each state is, independently of actions\n",
    "    def utilityFunction(\n",
    "        self\n",
    "    ):\n",
    "        return(output)\n",
    "\n",
    "\n",
    "    #The reward for transitioning between states, including rewards or costs for actions\n",
    "    def reward(\n",
    "        self,\n",
    "    ):\n",
    "       \n",
    "        return()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9b63d-30ae-42cb-895e-e576f38a43a1",
   "metadata": {},
   "source": [
    "## Agent Handler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45891f9e-66a5-47c9-90f4-d27097f44aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgentHandler:\n",
    "    def __init__(self, agent):\n",
    "        self.agent = agent\n",
    "\n",
    "    #Briefly test the agents to check performance is as expected. Should require minimal tuning, timesteps is the main thing\n",
    "    def evaluateAgent(self, timesteps = 60):\n",
    "        inputs = []\n",
    "        outputs = []\n",
    "        data = []\n",
    "        systemState = System().generateRandomState() #Consider replacing with generateRandomMiddleState() if appropriate\n",
    "        for i in tqdm(range(timesteps)):\n",
    "            inputs.append(systemState.readInputs())\n",
    "            out = self.agent.predict(systemState)\n",
    "            outputs.append(out)\n",
    "            data.append(systemState.readData())\n",
    "            systemState = systemState.setAction(*out)\n",
    "            systemState.updateSystem()\n",
    "        return([inputs, outputs, data])\n",
    "\n",
    "    #Show a graph for the performances in evaluateAgent\n",
    "    def displayEvaluations(self):\n",
    "        data = self.evaluateAgent()\n",
    "        for i in range(len(data)):\n",
    "            row = data[i]\n",
    "            plt.xlim(0, len(row))\n",
    "            plt.grid(True)\n",
    "            plt.plot(row)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "\n",
    "    \n",
    "    def processData(self, data):\n",
    "        outputs = []\n",
    "        for row in data:\n",
    "            subRow = []\n",
    "            transRow = np.array(row).T\n",
    "            for element in transRow:\n",
    "                subRow.append(np.percentile(transRow, 0))\n",
    "                subRow.append(np.percentile(transRow, 25))\n",
    "                subRow.append(np.median(transRow))\n",
    "                subRow.append(np.percentile(transRow, 75))\n",
    "                subRow.append(np.percentile(transRow, 100))\n",
    "            outputs.append(transRow)\n",
    "        outputs = np.array(outputs)\n",
    "        return(outputs.T)\n",
    "\n",
    "    #fullTest is a longer test of the agents. Not needed for all problems, and requires specific tuning\n",
    "    def fullTest(self):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
